{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Selecting Salient Features\n",
    "\n",
    "In the class you saw that there are two kinds of features: \n",
    "i) collocational and \n",
    "ii) bag of words. \n",
    "\n",
    "Having seen how to extract collocational features (by hand) to disambiguate the sense for a word with multiple senses in the previous problem, now let’s try to understand which features are important to disambiguate the word senses on a bigger corpus. You can use a combination of collocational and bag of words features to conduct feature selection.\n",
    "Dataset: We will use the English Lexical Sample task from Senseval for this problem. The data files for this project are available here: https://bit.ly/2kKEgwx\n",
    "\n",
    "It consists of i) a corpus file (wsd data.xml) and ii) a dictionary (dict.xml) that describes commonly used senses for each word. Both these files are in XML format. Every lexical item in the dictionary file contains multiple sense items, and each instance in the training data is annotated with the correct sense of the target word for a given context. The file wsd data.xml contains several <welt> tags corresponding to each word in the corpus. Each <welt> tag has an attribute item, whose value is “word.pos”, where “word” is the target word and “pos” represents the part-of-speech of the target word. Here ‘n’, ‘v’, and ‘a’ stand for noun, verb, and adjective, respectively. Each <welt> tag has several <instance> tags, each corresponds to an instance for the word that corresponds to the parent <welt> tag. Each <welt> tag also has an id attribute and contains one or more <ans> and a <context> tag. Every <ans> tag has two attributes, instance and senseid. The senseid attribute identifies the correct sense from the dictionary for the present word in the current context. A special value “U” is used to indicate if the correct sense is unclear. You can discard such instances from your feature extraction process for this assignment (we keep these cases so that you can take a look and think about how they can be utilized as well for realworld applications).\n",
    "\n",
    "A <context> tag contains:\n",
    "prev-context <head> target-word <head> next-context\n",
    "1. prev-context is the actual text given before the target word\n",
    "2. head is the actual appearance of the target word. Note that it may be morphological variant\n",
    "of the target word. For example, the word “begin.v” could show up as “beginning” instead\n",
    "of “begin” (lemma).\n",
    "3. next-context is the actual text that follows the target word.\n",
    "\n",
    "The dictionary file simply contains a gloss field for every sense item to indicate the corresponding definition. Each gloss consists of commonly used definitions delimited by a semicolon, and may have multiple real examples wrapped by quotation marks being also delimited by a semicolon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.1 Feature extraction\n",
    "Your first task is to extract features from the aforementioned corpus.   \n",
    "(1) Start with bag-of-word features and collocation features (define your own window size, see Hints below).  \n",
    "(2) Design new type of features. Submit the code and output for both.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relavant_text_and_pos_tags(text, focus_elements, window_size, y_label):\n",
    "    \n",
    "#     print(text)\n",
    "    # Get word tokens\n",
    "    word_tokens = word_tokenize(text)\n",
    "\n",
    "    #Get POS tags\n",
    "    pos_tags = pos_tag(word_tokens)\n",
    "    \n",
    "    all_heads_extracted = 0\n",
    "    # Token Iterator\n",
    "    i = 0\n",
    "    # Head Iterator\n",
    "    head = 0\n",
    "    \n",
    "    head_index_list = []\n",
    "    while head != len(focus_elements):\n",
    "#         print(focus_elements[head].text, i, word_tokens[i])\n",
    "        if word_tokens[i] == focus_elements[head].text:\n",
    "            if (i >= window_size) & (i < len(word_tokens)-window_size):\n",
    "                head_index_list.append((head, i))\n",
    "                head = head + 1\n",
    "        i = i + 1\n",
    "    \n",
    "    result = []\n",
    "    for head, index in head_index_list:\n",
    "        result_dict = {}\n",
    "        \n",
    "        previous_context_words = word_tokens[index-window_size:index]\n",
    "        previous_context_tags = pos_tags[index-window_size:index]\n",
    "        previous_context_bigrams = previous_context_words[-1] + \" \" + word_tokens[index]\n",
    "        \n",
    "        next_context_words = word_tokens[index+1:index+window_size+1]\n",
    "        next_context_tags = pos_tags[index+1:index+window_size+1]\n",
    "        next_context_bigrams = word_tokens[index] + \" \" + next_context_words[0]\n",
    "        \n",
    "        result_dict['text'] = word_tokens[index-window_size:index+window_size+1]\n",
    "        result_dict['previous_context'] = {\n",
    "            'words': previous_context_words,\n",
    "            'tags': previous_context_tags,\n",
    "            'bigrams':previous_context_bigrams\n",
    "        }\n",
    "        \n",
    "        result_dict['head'] = {\n",
    "            'words': word_tokens[index],\n",
    "            'tags': pos_tags[index]\n",
    "        }\n",
    "        \n",
    "        result_dict['next_context'] = {\n",
    "            'words': next_context_words,\n",
    "            'tags': next_context_tags,\n",
    "            'bigrams':next_context_bigrams\n",
    "        }\n",
    "        \n",
    "        result_dict['y'] = y_label\n",
    "        \n",
    "        result.append(result_dict)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wsd_data(input_file):\n",
    "    result = {}\n",
    "    # read file\n",
    "    with open(input_file) as file_object:\n",
    "        content = file_object.read()\n",
    "\n",
    "    # clean the content\n",
    "    content = re.sub('\\n*', '', content)\n",
    "\n",
    "    # parse content\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    # iterate through all welt elements\n",
    "    for welt in soup.find_all('welt'):\n",
    "        parsed_result_list = []\n",
    "        item_name = welt.get('item')\n",
    "        print(\"Parsing welt: {}\".format(item_name))\n",
    "        for instance in welt:\n",
    "            # Getting only the first sense id\n",
    "            senseid = instance.findAll('ans')[0].get('senseid')\n",
    "            \n",
    "            # Ignoring confusing tags\n",
    "            if senseid != \"U\":\n",
    "                # Get the label\n",
    "                y = item_name + '_' + senseid\n",
    "\n",
    "                # Get context\n",
    "                context = instance.find('context')\n",
    "\n",
    "                # Get focus elements\n",
    "                focus_elements = instance.findAll('head')\n",
    "                parsed_result = get_relavant_text_and_pos_tags(context.text, focus_elements, 2, y)\n",
    "                parsed_result_list.extend(parsed_result)\n",
    "                \n",
    "        result[item_name] = parsed_result_list\n",
    "                \n",
    "    #             break\n",
    "#         break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(instance_list, windows_size):\n",
    "    bow_tokens = []\n",
    "    bigrams_tokens = []\n",
    "    \n",
    "    # Get all vocab and bigrams (extra feature)\n",
    "    for i in range(len(instance_list)):\n",
    "        instance = instance_list[i]\n",
    "#         print(instance)\n",
    "        bow_tokens.extend(instance['previous_context']['words'])\n",
    "        bow_tokens.extend(instance['next_context']['words'])\n",
    "        \n",
    "        bigrams_tokens.append(instance['previous_context']['bigrams'])\n",
    "        bigrams_tokens.append(instance['next_context']['bigrams'])\n",
    "        \n",
    "    bow = Counter(bow_tokens)\n",
    "    bigrams = Counter(bigrams_tokens)\n",
    "    \n",
    "    # Get collocation features names\n",
    "    collocation_features_names = []\n",
    "    for i in range(1, windows_size+1):\n",
    "        collocation_features_names.append('pos-'+str(i))\n",
    "        collocation_features_names.append('pos'+str(i))\n",
    "    \n",
    "    # Create column names\n",
    "    column_names = list(bow.keys()) + list(bigrams.keys()) + collocation_features_names + ['y']\n",
    "    \n",
    "    print(\"Create dataframe: {}\".format(len(instance_list)))\n",
    "    # Create dataframe\n",
    "    df = pd.DataFrame(columns=column_names)\n",
    "    \n",
    "    # Populate dataframe\n",
    "    for i in range(len(instance_list)):\n",
    "        instance = instance_list[i]\n",
    "        df.loc[i, column_names] = [0] * len(column_names)\n",
    "        # Populate the bag of word tokens\n",
    "        temp_bow_tokens = instance['previous_context']['words'] + instance['next_context']['words']\n",
    "        for temp_bow_token in temp_bow_tokens:\n",
    "            df.loc[i, temp_bow_token] = 1\n",
    "        \n",
    "        # Populate the bigrams tokens\n",
    "        temp_bigrams_tokens = [instance['previous_context']['bigrams']] + [instance['next_context']['bigrams']]\n",
    "        for temp_bigram_token in temp_bigrams_tokens:\n",
    "            df.loc[i, temp_bigram_token] = 1\n",
    "        \n",
    "        # Populate the pos tags\n",
    "        for j in range(1, windows_size+1):\n",
    "            df.loc[i, 'pos-'+str(j)] = instance['previous_context']['tags'][-j][1]\n",
    "            df.loc[i, 'pos'+str(j)] = instance['next_context']['tags'][j-1][1]\n",
    "        \n",
    "        # Set y label\n",
    "        df.loc[i, 'y'] = instance['y']\n",
    "    \n",
    "    print(\"Hot encode dataframe\")\n",
    "    # Hot encode the categorical variables\n",
    "    df = pd.get_dummies(df, columns=collocation_features_names, prefix=collocation_features_names)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing welt: activate.v\n",
      "Parsing welt: add.v\n",
      "Parsing welt: appear.v\n",
      "Parsing welt: argument.n\n",
      "Parsing welt: arm.n\n",
      "Parsing welt: ask.v\n",
      "Parsing welt: atmosphere.n\n",
      "Parsing welt: audience.n\n",
      "Parsing welt: bank.n\n",
      "Parsing welt: begin.v\n",
      "Parsing welt: climb.v\n",
      "Parsing welt: decide.v\n",
      "Parsing welt: degree.n\n",
      "Parsing welt: difference.n\n",
      "Parsing welt: different.a\n",
      "Parsing welt: difficulty.n\n",
      "Parsing welt: disc.n\n",
      "Parsing welt: eat.v\n",
      "Parsing welt: encounter.v\n",
      "Parsing welt: expect.v\n",
      "Parsing welt: express.v\n",
      "Parsing welt: hear.v\n",
      "Parsing welt: hot.a\n",
      "Parsing welt: image.n\n",
      "Parsing welt: important.a\n",
      "Parsing welt: interest.n\n",
      "Parsing welt: judgment.n\n",
      "Parsing welt: lose.v\n",
      "Parsing welt: mean.v\n",
      "Parsing welt: miss.v\n",
      "Parsing welt: note.v\n",
      "Parsing welt: operate.v\n",
      "Parsing welt: organization.n\n",
      "Parsing welt: paper.n\n",
      "Parsing welt: party.n\n",
      "Parsing welt: performance.n\n",
      "Parsing welt: plan.n\n",
      "Parsing welt: play.v\n",
      "Parsing welt: produce.v\n",
      "Parsing welt: provide.v\n",
      "Parsing welt: receive.v\n",
      "Parsing welt: remain.v\n",
      "Parsing welt: rule.v\n",
      "Parsing welt: shelter.n\n",
      "Parsing welt: simple.a\n",
      "Parsing welt: smell.v\n",
      "Parsing welt: solid.a\n",
      "Parsing welt: sort.n\n",
      "Parsing welt: source.n\n",
      "Parsing welt: suspend.v\n",
      "Parsing welt: talk.v\n",
      "Parsing welt: treat.v\n",
      "Parsing welt: use.v\n",
      "Parsing welt: wash.v\n",
      "Parsing welt: watch.v\n",
      "Parsing welt: win.v\n",
      "Parsing welt: write.v\n"
     ]
    }
   ],
   "source": [
    "# dict_data = load_data('../input/WSD/dict.xml')\n",
    "# wsd_data = load_wsd_data('../input/WSD/wsd_data_custom.xml')\n",
    "wsd_data = load_wsd_data('../input/WSD/wsd_data.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional feature - Bigrams including the head word  \n",
    "  \n",
    "Window Size - +/-2, the reason for using this window size, smaller window size gives lesser number of features to encode leading to less sparsity in comparision if you use very large windows. Most of the times +/-2 should be sufficient to start with.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2 Feature selection\n",
    "\n",
    "Now, with the extracted features, perform feature selection to list top 10 features that would be most important to disambiguate a word sense. (1) Design your own feature selection algorithm and explain the intuition. (2) List the top 10 features in your answer key and also provide your code for this task. Submit the code and output for both.\n",
    "\n",
    "You can use following resources to read ways to perform feature selection:\n",
    "https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "https://www.datacamp.com/community/tutorials/feature-selection-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_top_k_fetures(x, y, k):\n",
    "    # Create and fit selector\n",
    "    selector = SelectKBest(chi2, k)\n",
    "    selector.fit(x, y)\n",
    "    \n",
    "    # Get columns(index) to keep and map to corresponding column names\n",
    "    column_indices = selector.get_support(indices=True)\n",
    "    selected_column_names = []\n",
    "    for column_index in column_indices:\n",
    "        selected_column_names.append(x.columns[column_index])\n",
    "\n",
    "    # Create new dataframe with only desired columns\n",
    "    x_new = x.loc[:, selected_column_names]\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activate.v\n",
      "Creating features\n",
      "Create dataframe: 230\n",
      "Hot encode dataframe\n",
      "Selecting features\n",
      "add.v\n",
      "Creating features\n",
      "Create dataframe: 264\n",
      "Hot encode dataframe\n",
      "Selecting features\n",
      "appear.v\n",
      "Creating features\n",
      "Create dataframe: 267\n"
     ]
    }
   ],
   "source": [
    "features_output_file = open('../output/features.txt', 'w')\n",
    "features_select_output_file = open('../output/features_select.txt', 'w')\n",
    "for key in wsd_data.keys():\n",
    "    print(key)\n",
    "    # Create features and feature vectors\n",
    "    print(\"Creating features\")\n",
    "    df = create_features(wsd_data[key], 2)\n",
    "    x = df[df.columns.difference(['y'])]\n",
    "    y = df[['y']]\n",
    "    features_output_file.write('{}: '.format(key))\n",
    "    for column in x.columns:\n",
    "         features_output_file.write('{},'.format(column))\n",
    "    features_output_file.write('\\n\\n')\n",
    "    \n",
    "    # Store feature vector and corresponding y as a csv file\n",
    "    x.to_csv('../output/'+key+'_x.csv', index=False)\n",
    "    y.to_csv('../output/'+key+'_y.csv', index=False)\n",
    "    \n",
    "    # Select top k features\n",
    "    print(\"Selecting features\")\n",
    "    x_new = select_top_k_fetures(x, y, 10)\n",
    "    features_select_output_file.write('{}: {}\\n\\n'.format(key, x_new.columns))\n",
    "    x_new.to_csv('../output/'+key+'_fs_x.csv', index=False)\n",
    "    \n",
    "features_output_file.close()\n",
    "features_select_output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select features, i am using SelectKBest functionality from scikit-learn which takes in an argument the function to be used to select features. In this case, i have choses chi2 as the selection algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "word_sense_disambiguation",
   "language": "python",
   "name": "word_sense_disambiguation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
